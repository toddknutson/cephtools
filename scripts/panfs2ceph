#!/bin/bash



#######################################################################
# Introduction
#######################################################################
# "panfs2ceph" is the name of this script. It is an archiving tool to 
# copy a single directory from tier 1 (panfs) storage to tier 2 (ceph). 
#
# This script examines a single dir and creates a slurm jobscript to copy the data
# from panfs to ceph (MSI's tier 2 storage). It also creates a second slurm jobscript
# that will delete the data from panfs. Finally, it creates a third slurm jobscript 
# that will copy the data from ceph back to panfs, restoring the original data
# back to the original location.
#
# It was developed by Todd Knutson (knut0297@umn.edu).
VERSION=1.0




#######################################################################
# License
#######################################################################
# "panfs2ceph"
# Copyright (C) 2020 University of Minnesota
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.





#######################################################################
# Parse input arguments
#######################################################################

# Default values:
dry_run=""
verbose_mode=""
remote=""
bucket=""
threads=1      
delete_empty_dirs="FALSE"


read -r -d '' help_message << EOF > /dev/null
---------------------------------------------------------------------
panfs2ceph

DESCRIPTION: 
Archiving tool to copy a single directory from tier 1 (panfs) storage to tier 2 (ceph). 

USAGE EXAMPLES:
panfs2ceph [options] -r REMOTE -b BUCKET dirname

OPTIONS:
-d              Dry run. Nothing transfered or deleted.
-v              Verbose mode.
-e              Do NOT transfer empty dirs on panfs to ceph. [Default is to create a hidden file (".empty_dir") inside all empty dirs so they get transfered to ceph. Setting this flag will not create the ".empty_dir" files, thus empty dirs will not get copied to ceph.]
-r STRING       Rclone remote name. (use "rclone listremotes" for available remotes)
-b STRING       Bucket name.
-t INT          Threads to use for uploading. [Default = 1].
-h              Print this help screen.

VERSION: ${VERSION}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/panfs2ceph
---------------------------------------------------------------------
EOF



# ---------------------------------------------------------------------
# Parse options
# ---------------------------------------------------------------------

# If run with zero input args, or using alternate help flags, print help message
if [[ $# == 0 ]]
then
    echo "$help_message"
    exit 0
fi

echo "[panfs2ceph "$(date)"] Starting ..."


# https://stackoverflow.com/a/30026641/2367748
# Transform long options to short ones
for arg in "$@"; do
    shift
    case "$arg" in
        "--help") set -- "$@" "-h" ;;
        "-help") set -- "$@" "-h" ;;
        "--dryrun") set -- "$@" "-d" ;;
        "--verbose_mode") set -- "$@" "-v" ;;
        "--verbose") set -- "$@" "-v" ;;
        "--delete_empty_dirs") set -- "$@" "-e" ;;    
        "--remote") set -- "$@" "-r" ;;
        "--bucket") set -- "$@" "-b" ;;
        "--threads") set -- "$@" "-t" ;;
        *) set -- "$@" "$arg"
    esac
done

while getopts ":hdfver:b:t:" opt; do
    case $opt in
        h) echo "$help_message"; exit 0 ;;
        d) dry_run="--dry-run" ;;
        v) verbose_mode="-v" ;;
        e) delete_empty_dirs="TRUE" ;;
        r) remote="$OPTARG" ;;
        b) bucket="$OPTARG" ;;
        t) threads="$OPTARG" ;;
        :) echo "Error: option ${OPTARG} requires a value" >&2; exit 64 ;;
        *) echo "Error: option ${OPTARG} is not implemented" >&2; exit 64 ;;
        \?) echo "Invalid option: ${OPTARG}" >&2; exit 64 ;;
    esac
done
shift $((OPTIND-1))


if [[ ! $remote ]]
then
    echo "The '--remote' option must be specified." >&2
    exit 64
fi
if [[ ! $bucket ]]
then
    echo "The '--bucket' option must be specified." >&2
    exit 64
fi




# ---------------------------------------------------------------------
# Parse and capture positional args
# ---------------------------------------------------------------------



# The "named" arguments above need to have a flag preceding them.
# The remaining argument is given without a flag -- i.e. so that bash brace expansion can be used or the unix find output.

for i in "$@"; do
    positional_arg+=("$i")
    positional_arg_readlink+=(`readlink -m "$i"`)
done



# ---------------------------------------------------------------------
# Check to make sure one, and only one, dir name was given as a positional arg
# ---------------------------------------------------------------------



# The "positional" input arguments must only be one pathname to a directory, no more or less.
if [[ ${#positional_arg_readlink[@]} -ne 1 ]]
then
    echo "
    check_file_path help:
    One and only one argument is allowed.
    " >&2
    exit 1
fi

root_path_dir=$positional_arg_readlink



# If the positional arg is not a directory, exit
if [[ -d $root_path_dir ]]; then
    # echo "dir: $root_path_dir"
    :
elif [[ -f $root_path_dir ]]; then
    echo "file: $root_path_dir" >&2
    echo "file inputs are not allowed." >&2
    exit 1
else
    echo "not valid: $root_path_dir" >&2
    echo "only dirname/path inputs are allowed." >&2
    exit 1
fi


# ---------------------------------------------------------------------
# Check to make sure this input dir does NOT already exist on ceph.
# ---------------------------------------------------------------------

module load /home/lmnp/knut0297/software/modulesfiles/rclone/1.53.2

# Count the number of ceph objects (files) listed under the input archive dir. If there are any files under this dir name already, stop.
remote_root_path_dir_filenumber=$(rclone lsf ${remote}:${bucket}$(dirname ${root_path_dir})/$(basename ${root_path_dir}) --max-depth 1 | wc -l)


if [[ $remote_root_path_dir_filenumber -gt 0 ]]
then
    echo "Tier 2 (ceph) already has files saved under this input 'directory name' ($root_path_dir) in this bucket ($bucket). To prevent any possible over-writing of data on ceph, please choose a different panfs directory path to archive or a different ceph bucket."  >&2
    exit 75
else 
    #echo "Ceph does not already has this dir"
    :
fi







# ---------------------------------------------------------------------
# Create archive working dir
# ---------------------------------------------------------------------


myprefix=panfs2ceph_archive_$(date +"%Y-%m-%d-%H%M%S")
myprefix_dir=$(dirname $root_path_dir)/$(basename $root_path_dir)___$myprefix


mkdir -p $myprefix_dir
cd $myprefix_dir



# ---------------------------------------------------------------------
# Get a file list
# ---------------------------------------------------------------------




if [[ $delete_empty_dirs == "TRUE" ]]
then
    # "The '--delete_empty_dirs' option was specified. Any empty dirs will not be copied to ceph."
    :
else 
    # Create a hidden file inside all empty dirs. This will ensure the empty dir will get copied as an object to ceph.
    find $root_path_dir -type d -empty -print0 | xargs -I{} -0 touch {}/.empty_dir
fi



# Print a list of files that will be archived
echo "[panfs2ceph "$(date)"] Creating the archive file list. This might take a while for large dirs..."

rclone lsf -R $root_path_dir > $myprefix.filelist.txt
# myprefix the files with full pathname
sed -i -e "s|^|$root_path_dir/|" $myprefix.filelist.txt






# ---------------------------------------------------------------------
# Check for filenames or pathnames that are too long
# ---------------------------------------------------------------------

# On object storage (S3, ceph), filenames are irrelevant. Everything is a filename (with dirs being 
# represented by slash delimiters). Thus, we only need to test for path length.
# https://stackoverflow.com/questions/6870824/what-is-the-maximum-length-of-a-filename-in-s3

pathname_max=1024

# Are there any with filenames that are too long? They need to be less than 1024 characters (I think?).
awk '{ PATHNAME=$0; print length(PATHNAME)"\t"PATHNAME}' $myprefix.filelist.txt | awk -v pathname_max=$pathname_max -v out_file="$myprefix.filelist.paths_too_long.txt" '{if ($1 >= pathname_max) {print $0 > out_file}}'


if [[ -s $myprefix.filelist.paths_too_long.txt ]]
then
    echo "[panfs2ceph "$(date)"] Some pathnames are too long (> $pathname_max char) for ceph. See $myprefix.filelist.paths_too_long.txt for a list. Shorten them before proceeding." >&2
    exit 88
fi




#######################################################################
# Copy
#######################################################################


tee $myprefix.1_copy.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=$threads
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "["\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


root_path_dir=$root_path_dir
myprefix=$myprefix
myprefix_dir=$myprefix_dir
dry_run=$dry_run
verbose_mode=$verbose_mode
remote=$remote
bucket=$bucket
threads=$threads



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------



module load /home/lmnp/knut0297/software/modulesfiles/rclone/1.53.2




# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------



# Copy the files to ceph
rclone copy \$dry_run \$verbose_mode / \${remote}:\${bucket} --files-from-raw "\${myprefix}.filelist.txt" --transfers \$threads --s3-chunk-size 50M --s3-upload-concurrency 10

if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "rclone copy process: FAIL" 2>&1 | tee \$myprefix.COPY_FAILED
else
    echo "rclone copy process: SUCCESS"
fi




# ---------------------------------------------------------------------
# Verify
# ---------------------------------------------------------------------

# Get a file list from ceph (after the transfer)
rclone lsf -R \$dry_run \$verbose_mode \${remote}:\${bucket}\${root_path_dir} > \$myprefix.filelist_from_ceph.txt

# myprefix the files with full pathname
sed -i -e "s|^|\$root_path_dir/|" \$myprefix.filelist_from_ceph.txt




# Compare the sorted files on ceph (just uploaded) against the list of files from panfs
comm -23 <(sort \$myprefix.filelist.txt) <(sort \$myprefix.filelist_from_ceph.txt) > \$myprefix.files_on_panfs_but_not_ceph.txt

comm -13 <(sort \$myprefix.filelist.txt) <(sort \$myprefix.filelist_from_ceph.txt) > \$myprefix.files_on_ceph_but_not_panfs.txt


if [[ ! -s \$myprefix.files_on_panfs_but_not_ceph.txt ]]
then
   # The file above is empty. There are no files in the *.filelist.txt that are missing from ceph
   rm -rf \$myprefix.files_on_panfs_but_not_ceph.txt
else
    echo "There are files listed in *.filelist.txt that are not found on ceph. Review: \$myprefix.files_on_panfs_but_not_ceph.txt" >&2
fi


if [[ ! -s \$myprefix.files_on_ceph_but_not_panfs.txt ]]
then
   # The file above is empty. There are no files on ceph that are not listed in *.filelist.txt
   rm -rf \$myprefix.files_on_ceph_but_not_panfs.txt
else
    echo "There are files on ceph that are not listed in *.filelist.txt. Review: \$myprefix.files_on_ceph_but_not_panfs.txt" >&2
fi







# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "["\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







EOF







#######################################################################
# Delete
#######################################################################


tee $myprefix.2_delete.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "["\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


root_path_dir=$root_path_dir
myprefix=$myprefix
myprefix_dir=$myprefix_dir
dry_run=$dry_run
verbose_mode=$verbose_mode



# ---------------------------------------------------------------------
# Software
# ---------------------------------------------------------------------


module load /home/lmnp/knut0297/software/modulesfiles/rsync/3.1.2


# ---------------------------------------------------------------------
# Delete files from panfs
# ---------------------------------------------------------------------

# Delete all files in the file list
# Use rsync to delte files because it's faster than "rm"

empty_dir=\$myprefix_dir/empty_dir

mkdir -p \$empty_dir

rsync \$dry_run \$verbose_mode -a --force --delete \${empty_dir}/ \${root_path_dir}/



if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "Delete process: FAIL" 2>&1 | tee \$myprefix.DELETE_FAILED
else
    echo "Delete process: SUCCESS"
fi

# Delete the original archive dir and the temp empty dir
rm -rf \${root_path_dir}
rm -rf \$empty_dir





# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "["\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



EOF







#######################################################################
# Restore
#######################################################################


tee $myprefix.3_restore.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=$threads
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "["\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


root_path_dir=$root_path_dir
myprefix=$myprefix
myprefix_dir=$myprefix_dir
dry_run=$dry_run
verbose_mode=$verbose_mode
remote=$remote
bucket=$bucket
threads=$threads



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------



module load /home/lmnp/knut0297/software/modulesfiles/rclone/1.53.2



# ---------------------------------------------------------------------
# Check if original dir still exists on panfs
# ---------------------------------------------------------------------

if [ -d "\$root_path_dir" ]
then
    echo "The original dir (\$root_path_dir) still exists on panfs. Stopping restore process. Change the dir name on panfs and re-run restore script." >&2
    exit 1
fi



# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------

# Copy the files from ceph back to panfs
rclone copy \$dry_run \$verbose_mode \${remote}:\${bucket}\$root_path_dir \$root_path_dir --transfers \$threads --s3-chunk-size 50M --s3-upload-concurrency 10


if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "rclone restore process: FAIL" 2>&1 | tee \$myprefix.RESTORE_FAILED
else
    echo "rclone restore process: SUCCESS"
fi



# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "["\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







EOF






#######################################################################
# Readme
#######################################################################



tee $myprefix.readme.md << EOF > /dev/null


# panfs2ceph archive

## Introduction

This directory (\`$root_path_dir\`) was archived from MSI's \`panfs\` (tier 1) storage to its \`ceph\` tier 2 storge. All data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. After the data was copied, it was deleted from \`panfs\`. 


## How this works

* A program called \`panfs2ceph\` found all the files in a directory to archive and created a file list text file.
* Then the program created a slurm job file (bash script) that can be run to copy all files from panfs to ceph.
* The program also created a slurm job file (bash script) to delete the original data from panfs.
* The program also created this README file.
* The program does not actually run these slrum job scripts. You must do that manually after reviewing the file list, etc.


## Notes:

* A new dir is created next to the original that contains these archive-related files and scripts.



## Variables


Variable Name | Value 
-----------|----------
Directory to archive | $root_path_dir
Directory name for archive files | $myprefix
Directory path for archive files | $myprefix_dir
Ceph UserID | $(s3info info | grep username | awk -F': ' '{print $2}')
Ceph Remote Name | $remote
Ceph Bucket Name | $bucket
User doing transfer | $USER


## How to access the original data

### Browse the data on ceph

You can use a variety of tools to browse data stored on ceph. For example:

1. rclone (e.g. \`rclone ls ${remote}:${bucket}$root_path_dir\`). This requires a properly configured \`~/.config/rclone/rclone.conf\` file. 
2. s3cmd (e.g. \`s3cmd ls -r s3://${bucket}$root_path_dir\`). This requires a properly configured \`~/.s3cfg\` file. 
3. GUI based file browser (e.g. FileZilla, Panic's Transmit, CyberDuck, etc.)
 
 
### Browse the data via panfs \`snapshot\`

The original data was deleted, but might still be stored on \`panfs\` as a snapshot for a short period of time (approximately 1 month or less). If this data was archived to ceph and deleted from panfs, it might be recovered from panfs for a short time. Review the available snapshots here:

    ${root_path_dir}/.snapshot


### Restore from ceph back to panfs

Depending on user permissions, the data can be copied from ceph back to panfs using \`rclone\` or other methods. For example, running $myprefix.3_restore.slurm job script should copy the data from ceph back to panfs in its original location.





EOF




#######################################################################
# Print instructions to terminal
#######################################################################

echo "[panfs2ceph "$(date)"] Done."


read -r -d '' instructions_message << EOF > /dev/null
---------------------------------------------------------------------
panfs2ceph summary

Options used:
dry_run=$dry_run
verbose_mode=$verbose_mode
delete_empty_dirs=$delete_empty_dirs
remote=$remote
bucket=$bucket
threads=$threads

Archive dir: 
$root_path_dir

Archive dir transfer scripts:
$myprefix_dir

Achrive transfer files created -- but you're not done yet!
Next steps:
1. Move into transfer dir: cd $myprefix_dir
2. Review the *.readme.md file for details.
3. Review the *.filelist.txt file. The files in this list will be copied to ceph and delted from panfs.
4. Launch the copy jobfile: sbatch $myprefix.1_copy.slurm
5. After sucessful copy, launch the delete jobfile: sbatch $myprefix.2_delete.slurm
6. After the data has been deleted from panfs -- and you need it back in the same location, launch the restore jobfile: sbatch $myprefix.3_restore.slurm

VERSION: ${VERSION}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/panfs2ceph
---------------------------------------------------------------------
EOF


echo "$instructions_message"





