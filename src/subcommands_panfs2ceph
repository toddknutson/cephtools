


# ---------------------------------------------------------------------
# panfs2ceph
# ---------------------------------------------------------------------

describe "panfs2ceph" <<HEREDOC
---------------------------------------------------------------------
Usage:
    ${_ME} panfs2ceph [options] --remote <REMOTE> --bucket <BUCKET> --path <DIR_PATH>

Options:
    -r|--remote <STRING>    Rclone remote name. (use "rclone listremotes" for available
                            remotes). Rclone remotes must be set up using "rclone init"
                            and can be viewed at: ~/.config/rclone/rclone.config.
                            
    -b|--bucket <STRING>    Name of the ceph bucket that data should be used for the 
                            transfer.
    
    -p|--path <STRING>      Absolute or relative path to the directory that should be 
                            transfered.
    
    -d|--dry_run            Dry run option will be applied to rclone commands. Nothing 
                            transfered or deleted when scripts run.
    
    -v|--verbose            Verbose mode will be applied to rclone commands.
    
    -e|--delete_empty_dirs  Do NOT transfer empty dirs on panfs to ceph. [Default is to 
                            create a hidden file (".empty_dir") inside all empty dirs so 
                            they get transfered to ceph. Setting this flag will not create
                            the ".empty_dir" files, thus empty dirs will not get copied to
                            ceph.
                            
    -t|--threads <INT>      Threads to use for uploading with rclone. [Default = 1].
    
    --debug                 Print additional info when running this command (like verbose 
                            mode). 

Description:
  Archiving tool to copy a single directory from tier 1 (panfs) storage to tier 2 (ceph).
  
Help (print this screen):
    ${_ME} help panfs2ceph

Version: ${_VERSION}
Questions: Todd Knutson (knut0297@umn.edu)
GitHub: https://github.umn.edu/knut0297org/cephtools
---------------------------------------------------------------------
HEREDOC

panfs2ceph() {
#     echo ALL ARGS:
#     echo "${@:-}"
#     echo "${#}"


    
    # Parse Options ###############################################################

    # Initialize program option variables.
    local _USE_DEBUG=0
    local _bucket=
    local _remote=
    local _path=
    local _dry_run=
    local _verbose=
    local _delete_empty_dirs=0
    local _threads="1"

    # __get_option_value()
    #
    # Usage:
    #   __get_option_value <option> <value>
    #
    # Description:
    #  Given a flag (e.g., -e | --example) return the value or exit 1 if value
    #  is blank or appears to be another option.
    __get_option_value() {
      local __arg="${1:-}"
      local __val="${2:-}"
      
      if [[ -n "${__val:-}" ]] && [[ ! "${__val:-}" =~ ^- ]]
      then
        printf "%s\\n" "${__val}"
      else
        _exit_1 printf "%s requires a valid argument.\\n" "${__arg}"
      fi
    }


    # For flags (i.e. no corresponding value), do not shift inside the case testing
    # statement. For options with required value, shift inside case testing statement, 
    # so the loop moves twice. 
    while ((${#}))
    do
        __arg="${1:-}"
        __val="${2:-}"

        case "${__arg}" in
        --debug)
            _USE_DEBUG=1
            ;;
        -d|--dry_run)
            _dry_run="--dry-run"
            ;;
        -v|--verbose)
            _verbose="-v"
            ;;
        -e|--delete_empty_dirs)
            _delete_empty_dirs=1
            ;;
        -b|--bucket)
            _bucket="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -r|--remote)
            _remote="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -p|--path)
            _path="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        -t|--threads)
            _threads="$(__get_option_value "${__arg}" "${__val:-}")"
            shift
            ;;
        --endopts)
            # Terminate option parsing.
            break
            ;;
        -*)
            _exit_1 printf "Unexpected option: %s\\n" "${__arg}"
            ;;
        *)
            describe --get panfs2ceph
            _exit_1 printf "Unexpected positional arg: %s\\n" "${__arg}"
            ;;
        esac

        shift
    done


    # ---------------------------------------------------------------------
    # Check and print input options
    # ---------------------------------------------------------------------

    printf -- "Program options used:\\n"
    printf -- "--debug: %s\\n" "$_USE_DEBUG"
    printf -- "--bucket: %s\\n" "$_bucket"
    printf -- "--remote: %s\\n" "$_remote"
    printf -- "--path: %s\\n" "$_path"
    printf -- "--dry_run: %s\\n" "$_dry_run"
    printf -- "--verbose: %s\\n" "$_verbose"
    printf -- "--delete_empty_dirs: %s\\n" "$_delete_empty_dirs"
    printf -- "--threads: %s\\n" "$_threads"


    # If required options are empty or null, exit.
    if [ -z "${_remote}" ]
    then
        _exit_1 printf "The '--remote' option must be specified and not be empty or null."
    fi
    if [ -z "${_bucket}" ]
    then
        _exit_1 printf "The '--bucket' option must be specified and not be empty or null."
    fi
    if [ -z "${_path}" ]
    then
        _exit_1 printf "The '--path' option must be specified and not be empty or null."
    fi
    

    _root_path_dir=$(readlink -m "${_path}")
    if [ ! -d "${_root_path_dir}" ]
    then
        _exit_1 printf "The '--path' option specified is not a valid directory. \\nReadlink does not convert to a valid directory: 'readlink -m %s'\\n" "${_path}"
    fi
    

    # ---------------------------------------------------------------------
    # Check rclone specific info
    # ---------------------------------------------------------------------
    if command -v rclone &> /dev/null
    then
        printf "Using rclone found in PATH:\\n"
        printf "%s\\n" "$(command -v rclone)"
        printf "%s\\n" "$(rclone --version)" 
    else
        printf "rclone could not be found in PATH, so using the MSI module: %s\\n" "/panfs/roc/soft/modulefiles.common/rclone/1.54"
        module load rclone/1.54
        printf "%s\\n" "$(command -v rclone)"
        printf "%s\\n" "$(rclone --version)" 
    fi

    # Make sure the remote exists
    if ! rclone listremotes | grep -q "^$_remote:\$"
    then
        printf "Rclone remote does not exist: %s\\n" "$_remote"
        _exit_1 printf "Check available remotes by running 'rclone listremotes', or set one up by running 'rclone init'.\\n"
    fi

    # Make sure access to bucket is possible
    if ! rclone lsf ${_remote}:${_bucket}
    then
        _exit_1 printf "Errors occured when accessing bucket: '%s'\\nDo you have access rights to the bucket?\\nCheck the bucket access policy using 's3cmd info s3://%s'\\n" "${_bucket}" "${_bucket}"
    fi

    

    # ---------------------------------------------------------------------
    # Check to make sure this input dir does NOT already exist on ceph.
    # ---------------------------------------------------------------------


    # Count the number of ceph objects (files) listed under the input archive dir. If there are any files under this dir name already, stop.
    remote_root_path_dir_filenumber=$(rclone lsf ${_remote}:${_bucket}$(dirname ${_root_path_dir})/$(basename ${_root_path_dir}) --max-depth 1 2> /dev/null | wc -l)


    if [[ $remote_root_path_dir_filenumber -gt 0 ]]
    then
        _exit_1 printf "Tier 2 (ceph) already has files saved under this input 'directory name' (%s) in this bucket (%s). To prevent any possible over-writing of data on ceph, please choose a different panfs directory path to archive or a different ceph bucket.\\n" "${_root_path_dir}" "${_bucket}"
    fi



    # ---------------------------------------------------------------------
    # Create archive working dir
    # ---------------------------------------------------------------------

    _archive_date_time="$(date +"%Y-%m-%d-%H%M%S")"
    _myprefix="panfs2ceph_archive_${_archive_date_time}"
    _myprefix_dir=$(dirname ${_root_path_dir})/$(basename ${_root_path_dir})___${_myprefix}

    echo ${_myprefix_dir}
    mkdir -p ${_myprefix_dir}
    chmod --reference ${_root_path_dir} ${_myprefix_dir}
    cd ${_myprefix_dir}






    # ---------------------------------------------------------------------
    # Get a file list
    # ---------------------------------------------------------------------


    if ((_delete_empty_dirs))
    then
        # "The '--delete_empty_dirs' option was specified. Any empty dirs will not be copied to ceph."
        :
    else 
        # Create a hidden file inside all empty dirs. This will ensure the empty dir will get copied as an object to ceph.
        find ${_root_path_dir} -type d -empty -print0 | xargs -I{} -0 touch {}/.empty_dir
    fi



    # Print a list of files that will be archived
    echo "[panfs2ceph "$(date)"] Creating the archive file list. This might take a while for large dirs..."

    rclone lsf -R ${_root_path_dir} > ${_myprefix}.filelist.txt
    # _myprefix the files with full pathname
    sed -i -e "s|^|${_root_path_dir}/|" ${_myprefix}.filelist.txt






    # ---------------------------------------------------------------------
    # Check for filenames or pathnames that are too long
    # ---------------------------------------------------------------------

    # This check is probably pointless because I think the limit is the same for both
    # panfs and ceph -- so if files are on panfs, they should also fit on ceph.

    # On object storage (S3, ceph), filenames are irrelevant. Everything is a filename (with dirs being 
    # represented by slash delimiters). Thus, we only need to test for path length.
    # https://stackoverflow.com/questions/6870824/what-is-the-maximum-length-of-a-filename-in-s3

    pathname_max=1024

    # Are there any with filenames that are too long? They need to be less than 1024 characters (I think?).
    awk '{ PATHNAME=$0; print length(PATHNAME)"\t"PATHNAME}' ${_myprefix}.filelist.txt | awk -v pathname_max=$pathname_max -v out_file="${_myprefix}.filelist.paths_too_long.txt" '{if ($1 >= pathname_max) {print $0 > out_file}}'


    if [[ -s ${_myprefix}.filelist.paths_too_long.txt ]]
    then
        echo "[panfs2ceph "$(date)"] Some pathnames are too long (> $pathname_max char) for ceph. See ${_myprefix}.filelist.paths_too_long.txt for a list. Shorten them before proceeding." >&2
        exit 88
    fi




    #######################################################################
    # Copy
    #######################################################################


    tee ${_myprefix}.1_copy.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=$_root_path_dir
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_verbose=${_verbose}
_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

module load rclone/1.54




# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------



# Copy the files to ceph
rclone copy "/" "\${_remote}:\${_bucket}" --files-from-raw "\${_myprefix}.filelist.txt" \${_dry_run} \${_verbose} --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10

if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] Rclone copy process: FAIL" 2>&1 | tee \${_myprefix}.COPY_FAILED
else
    echo "[panfs2ceph "\$(date)"] Rclone copy process: SUCCESS"
fi




# ---------------------------------------------------------------------
# Verify
# ---------------------------------------------------------------------


# Sleep for a short time to allow rclone to properly list files. If done without
# a short break, rclone lsf seems fail.
echo "[panfs2ceph "\$(date)"] Starting verification..."
#echo "[panfs2ceph "\$(date)"] Waiting for 30 sec to allow ceph to index files..."
#sleep 30
echo "[panfs2ceph "\$(date)"] Getting file list from ceph..."

# Nudge rclone to list the files on bucket (refresh). This seems to help avoid random errors.
#rclone lsf -R \${_remote}: --max-depth 1 > /dev/null
#rclone lsf -R \${_remote}:\${_bucket} --max-depth 1 > /dev/null


# Get a file list from ceph (after the transfer)
rclone lsf -R \${_remote}:\${_bucket}\${_root_path_dir} > \${_myprefix}.filelist_from_ceph.txt

# _myprefix the files with full pathname
sed -i -e "s|^|\${_root_path_dir}/|" \${_myprefix}.filelist_from_ceph.txt




# Compare the sorted files on ceph (just uploaded) against the list of files from panfs
comm -23 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_panfs_but_not_ceph.txt

comm -13 <(sort \${_myprefix}.filelist.txt) <(sort \${_myprefix}.filelist_from_ceph.txt) > \${_myprefix}.files_on_ceph_but_not_panfs.txt


if [[ ! -s \${_myprefix}.files_on_panfs_but_not_ceph.txt ]]
then
   # The file above is empty.
   #echo "[panfs2ceph "\$(date)"] There are no files in the *.filelist.txt that are missing from ceph."
   rm -rf \${_myprefix}.files_on_panfs_but_not_ceph.txt
else
    echo "[panfs2ceph "\$(date)"] There are files listed in *.filelist.txt that are not found on ceph. Review: \${_myprefix}.files_on_panfs_but_not_ceph.txt" >&2
fi


if [[ ! -s \${_myprefix}.files_on_ceph_but_not_panfs.txt ]]
then
   # The file above is empty.
   #echo "[panfs2ceph "\$(date)"] There are no files on ceph that are not listed in *.filelist.txt."
   rm -rf \${_myprefix}.files_on_ceph_but_not_panfs.txt
else
    echo "[panfs2ceph "\$(date)"] There are files on ceph that are not listed in *.filelist.txt. Review: \${_myprefix}.files_on_ceph_but_not_panfs.txt" >&2
fi







# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







EOF







    #######################################################################
    # Delete
    #######################################################################


    tee ${_myprefix}.2_delete.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_verbose=${_verbose}



# ---------------------------------------------------------------------
# Software
# ---------------------------------------------------------------------

cd \${_myprefix_dir}

module load /home/lmnp/knut0297/software/modulesfiles/rsync/3.1.2


# ---------------------------------------------------------------------
# Delete files from panfs
# ---------------------------------------------------------------------

# Delete all files in the file list
# Use rsync to delte files because it's faster than "rm"

_empty_dir=\${_myprefix_dir}/empty_dir

mkdir -p \${_empty_dir}

rsync \${_dry_run} \${_verbose} -a --force --delete \${_empty_dir}/ \${_root_path_dir}/



if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] Delete process: FAIL" 2>&1 | tee \${_myprefix}.DELETE_FAILED
else
    echo "[panfs2ceph "\$(date)"] Delete process: SUCCESS"
fi

# Delete the original archive dir and the temp empty dir
rm -rf \${_root_path_dir}
rm -rf \${_empty_dir}





# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi



EOF







    #######################################################################
    # Restore
    #######################################################################


    tee ${_myprefix}.3_restore.slurm << EOF > /dev/null
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=${_threads}
#SBATCH --time=24:00:00
#SBATCH --mem=32gb
#SBATCH --error=%x.e%j
#SBATCH --output=%x.o%j
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=$USER@umn.edu
#SBATCH --partition=amdsmall

echo "[panfs2ceph "\$(date)"] Script start."


# ---------------------------------------------------------------------
# Variables
# ---------------------------------------------------------------------


_root_path_dir=${_root_path_dir}
_myprefix=${_myprefix}
_myprefix_dir=${_myprefix_dir}
_dry_run=${_dry_run}
_verbose=${_verbose}
_remote=${_remote}
_bucket=${_bucket}
_threads=${_threads}



# ---------------------------------------------------------------------
# Load software
# ---------------------------------------------------------------------



module load rclone/1.54



# ---------------------------------------------------------------------
# Check if original dir still exists on panfs
# ---------------------------------------------------------------------

if [ -d "\${_root_path_dir}" ]
then
    echo "[panfs2ceph "\$(date)"] The original dir (\${_root_path_dir}) still exists on panfs. Stopping restore process. Change the dir name on panfs and re-run restore script." >&2
    exit 1
fi



# ---------------------------------------------------------------------
# rclone
# ---------------------------------------------------------------------

# Nudge rclone to list the files on bucket (refresh). This seems to help avoid random errors.
rclone lsf -R \${_remote}:\${_bucket} --max-depth 1 > /dev/null


# Copy the files from ceph back to panfs
rclone copy "\${_remote}:\${_bucket}\${_root_path_dir}" "\${_root_path_dir}" \${_dry_run} \${_verbose} --transfers \${_threads} --s3-chunk-size 50M --s3-upload-concurrency 10


if [ ! \$? -eq 0 ]
then
    # rclone exit status was not zero
    echo "[panfs2ceph "\$(date)"] rclone restore process: FAIL" 2>&1 | tee \${_myprefix}.RESTORE_FAILED
else
    echo "[panfs2ceph "\$(date)"] rclone restore process: SUCCESS"
fi



# ---------------------------------------------------------------------
# Job summary info
# ---------------------------------------------------------------------

echo "[panfs2ceph "\$(date)"] Script end."

if [ ! -z \${SLURM_JOB_ID+x} ]; then
    scontrol show job "\${SLURM_JOB_ID}"
    sstat -j "\${SLURM_JOB_ID}" --format=JobID,MaxRSS,MaxVMSize,NTasks,MaxDiskWrite,MaxDiskRead
fi







EOF






    #######################################################################
    # Readme
    #######################################################################



    tee ${_myprefix}.readme.md << EOF > /dev/null


# panfs2ceph archive

Archive initated (Y-m-d-HMS):  
${_archive_date_time}   

## Introduction

This directory (\`${_root_path_dir}\`) was archived from MSI's \`panfs\` (tier 1) storage to its \`ceph\` tier 2 storge. All data inside the directory was copied using \`rclone\` which uses MD5 checksums on every transfer to ensure data integrity. After the data was copied, it was deleted from \`panfs\`. 


## How this works

* A program called \`panfs2ceph\` found all the files in a directory to archive and created a file list text file.
* Then the program created a slurm job file (bash script) that can be run to copy all files from panfs to ceph.
* The program also created a slurm job file (bash script) to delete the original data from panfs.
* The program also created this README file.
* The program does not actually run these slrum job scripts. You must do that manually after reviewing the file list, etc.


## Notes:

* A new dir is created next to the original that contains these archive-related files and scripts.
* File modification times/dates should be preserved between transfers. However, if files are transferred from ceph back to panfs, the original modification times of directories will be lost (i.e. block storage like tier 2 does not have directories, so their mod times can't be preserved).



## Variables


Variable Name | Value 
-----------|----------
Archive date and time | ${_archive_date_time}
Directory to archive | ${_root_path_dir}
Directory name for archive files | ${_myprefix}
Directory path for archive files | ${_myprefix_dir}
Ceph UserID | $(s3info info | grep username | awk -F': ' '{print $2}')
Ceph Remote Name | ${_remote}
Ceph Bucket Name | ${_bucket}
User doing transfer | $USER


## How to access the original data

### Browse the data on ceph

You can use a variety of tools to browse data stored on ceph. For example:

1. rclone (e.g. \`rclone ls ${_remote}:${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.config/rclone/rclone.conf\` file. 
2. s3cmd (e.g. \`s3cmd ls -r s3://${_bucket}${_root_path_dir}\`). This requires a properly configured \`~/.s3cfg\` file. 
3. GUI based file browser (e.g. FileZilla, Panic's Transmit, CyberDuck, etc.)


### Browse the data via panfs \`snapshot\`

The original data was deleted, but might still be stored on \`panfs\` as a snapshot for a short period of time (approximately 1 month or less). If this data was archived to ceph and deleted from panfs, it might be recovered from panfs for a short time. Review the available snapshots here:

    ${_root_path_dir}/.snapshot


### Restore from ceph back to panfs

Depending on user permissions, the data can be copied from ceph back to panfs using \`rclone\` or other methods. For example, running ${_myprefix}.3_restore.slurm job script should copy the data from ceph back to panfs in its original location.





EOF




    #######################################################################
    # Print instructions to terminal
    #######################################################################

    echo "[panfs2ceph "$(date)"] Done."


    #read -r -d '' instructions_message << EOF > /dev/null

    # The unix "read" command does not return a zero exit code. Use a temp function instead.
    # https://stackoverflow.com/a/8088167/2367748
    heredoc2var(){ IFS='\n' read -r -d '' ${1} || true; }
    
    heredoc2var instructions_message << EOF > /dev/null

---------------------------------------------------------------------
cephtools panfs2ceph summary

Options used:
dry_run=${_dry_run}
verbose=${_verbose}
delete_empty_dirs=${_delete_empty_dirs}
remote=${_remote}
bucket=${_bucket}
threads=${_threads}

Archive dir: 
${_root_path_dir}

Archive dir transfer scripts:
${_myprefix_dir}

Achrive transfer files created -- but you're not done yet!
Next steps:
1. Move into transfer dir: cd ${_myprefix_dir}
2. Review the *.readme.md file for details.
3. Review the *.filelist.txt file. The files in this list will be copied to ceph and delted from panfs.
4. Launch the copy jobfile: sbatch ${_myprefix}.1_copy.slurm
5. After sucessful copy, launch the delete jobfile: sbatch ${_myprefix}.2_delete.slurm
6. After the data has been deleted from panfs -- and you need it back in the same location, launch the restore jobfile: sbatch ${_myprefix}.3_restore.slurm

VERSION: ${_VERSION}
QUESTIONS: Todd Knutson (knut0297@umn.edu)
REPO: https://github.umn.edu/knut0297org/cephtools
---------------------------------------------------------------------
EOF


    echo "$instructions_message"


}



